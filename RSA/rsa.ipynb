{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88823c0a-e81c-4dec-85c4-138be4eedd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('rsa_primes.csv')\n",
    "\n",
    "with open('fine_tune.txt', 'w') as f:\n",
    "    for _, row in df.iterrows():\n",
    "        line = f\"n: {row['n']} -> p: {row['p']} q: {row['q']}\\n\"\n",
    "        f.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5437672-b2a5-4e9f-86c6-4a6fb7954163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ishaan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishaan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1385' max='1385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1385/1385 1:01:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.256800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('models/fine_tuned_gpt2\\\\tokenizer_config.json',\n",
       " 'models/fine_tuned_gpt2\\\\special_tokens_map.json',\n",
       " 'models/fine_tuned_gpt2\\\\vocab.json',\n",
       " 'models/fine_tuned_gpt2\\\\merges.txt',\n",
       " 'models/fine_tuned_gpt2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Load tokenizer and model from local path\n",
    "model_path = \"models/gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Load and tokenize dataset\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"fine_tune.txt\",\n",
    "    block_size=512,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"models/fine_tuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"models/fine_tuned_gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f2d02-3bd8-42c3-80f4-cf925280c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                | 324/2000 [46:09<4:01:47,  8.66s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sympy import isprime\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_df = pd.read_csv(\"rsa_primes_test.csv\")\n",
    "# Load fine-tuned model\n",
    "model_path = \"models/fine_tuned_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "def extract_digits(text):\n",
    "    return ''.join(filter(str.isdigit, text))\n",
    "\n",
    "def predict_factors(n_str, max_attempts=3):\n",
    "    prompt = f\"n: {n_str} ->\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    for _ in range(max_attempts):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_length=300,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.9,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        decoded = tokenizer.decode(outputs[0])\n",
    "        try:\n",
    "            p_raw = decoded.split(\"p:\")[1].split(\"q:\")[0].strip()\n",
    "            q_raw = decoded.split(\"q:\")[1].strip()\n",
    "\n",
    "            p_val = int(extract_digits(p_raw))\n",
    "            q_val = int(extract_digits(q_raw))\n",
    "\n",
    "            return p_val, q_val\n",
    "        except:\n",
    "            continue\n",
    "    return None, None\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    actual_p = int(str(row['p'])[1:])  # remove 'a' prefix\n",
    "    actual_q = int(str(row['q'])[1:])\n",
    "    n = str(row['n'])[1:]\n",
    "\n",
    "    pred_p, pred_q = predict_factors(n)\n",
    "\n",
    "    if pred_p is not None and pred_q is not None:\n",
    "        if {pred_p, pred_q} == {actual_p, actual_q}:\n",
    "            correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"\\n‚úÖ Model Accuracy: {accuracy:.2f}% on {total} test cases\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1834d0c-c8f6-411b-af04-89523807b2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Output: n: a26479028122147196027 -> p: a57896044618658097711785492504343953926634992332820282019729895530604748151885703 q: a578960446186580977117854925043439539266349923328202820197345642750367560452079\n",
      "n: a3351951982485649274893506249551461531869841455148098344431889089901641256272698008869603514787823233411898829772575676905146517234063295490989794828535099 -> p: a5789604461865809771178549250434395392663499233282028201973378581667289738963 q: a57896044618658097711785492504343953926634992332820282019740669816772938853907\n",
      "n: a33519519824856492748935062495514615318698414551480983444318791609551216058855603918151466459914281712448898127959257845139833\n",
      "\n",
      "‚úÖ Predicted primes:\n",
      "p = 57896044618658097711785492504343953926634992332820282019729895530604748151885703\n",
      "q = 578960446186580977117854925043439539266349923328202820197345642750367560452079\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from sympy import isprime\n",
    "\n",
    "model_path = \"models/fine_tuned_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from sympy import isprime\n",
    "\n",
    "model_path = \"models/fine_tuned_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def predict_factors(n_str, max_attempts=3):\n",
    "    prompt = f\"n: {n_str} ->\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    for _ in range(max_attempts):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_length=300,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.9,\n",
    "                do_sample=True,\n",
    "                top_k=50\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.decode(outputs[0])\n",
    "        print(f\"\\nüîç Output: {decoded}\")\n",
    "\n",
    "        # Try to extract p and q\n",
    "        if \"p:\" in decoded and \"q:\" in decoded:\n",
    "            try:\n",
    "                p_str = decoded.split(\"p:\")[1].split(\"q:\")[0].strip()\n",
    "                q_str = decoded.split(\"q:\")[1].split(\"\\n\")[0].strip()\n",
    "                # if isprime(int(p_str[1:])) and isprime(int(q_str[1:])):\n",
    "                #     return p_str, q_str\n",
    "                return p_str[1:], q_str[1:]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return None, None\n",
    "\n",
    "# Example usage\n",
    "\n",
    "n = \"a3351951982485649274893506249551461531869841455148098344431851084682782361479386449415440197611684937280096407359711927382771525195526408454081639758934933\"\n",
    "p_pred, q_pred = predict_factors(n)\n",
    "print(f\"\\n‚úÖ Predicted primes:\\np = {p_pred}\\nq = {q_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949429d2-189f-4c59-ab76-851f4e1437fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plaintext number: 805121215\n",
      "Encrypted ciphertext: 1265486306274259077781284030375601665475060877223086162653019628051397314338821524969155356865578468897576975544391903911393398918764660404040400316337347\n"
     ]
    }
   ],
   "source": [
    "# Given RSA public key components\n",
    "p = 57896044618658097711785492504343953926634992332820282019739415617116836954601\n",
    "q = 57896044618658097711785492504343953926634992332820282019734762335316498173133\n",
    "n = 3351951982485649274893506249551461531869841455148098344431851084682782361479386449415440197611684937280096407359711927382771525195526408454081639758934933\n",
    "e = 65537\n",
    "\n",
    "# Message to encrypt\n",
    "message = \"HELLO\"\n",
    "\n",
    "# Convert message to numbers (A=1 to Z=26), zero-padded to 2 digits each\n",
    "number_str = ''.join([f\"{ord(char)-64:02d}\" for char in message.upper()])\n",
    "m = int(number_str)\n",
    "\n",
    "print(\"Plaintext number:\", m)\n",
    "\n",
    "# Encrypt using RSA: c = m^e mod n\n",
    "c = pow(m, e, n)\n",
    "\n",
    "print(\"Encrypted ciphertext:\", c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "107399d8-c2f6-456a-9dc9-c01b278c6bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5789604461865809771178549250434395392663499233282028201973406588016424694789"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(q_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ed32421-3951-47a1-838f-c99ab9f8a646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciphertext: 1265486306274259077781284030375601665475060877223086162653019628051397314338821524969155356865578468897576975544391903911393398918764660404040400316337347\n",
      "Decrypted numeric message: 2459704585754303232621562479658355849468497433466205433760616098786060803207868904198188180366020197512663334662778258494037023157535533155655952152652\n",
      "Decrypted text message: BSSDFEBDFFJOJURFIFWTFSQHNTBGXFPIIHFHCTZPLOTRCBJHTSWLNGHNAZYFPCRWOWIAECMGZOZZ\n"
     ]
    }
   ],
   "source": [
    "from sympy import mod_inverse\n",
    "\n",
    "# Given RSA components\n",
    "p = int(p_pred)\n",
    "q = int(q_pred)\n",
    "# p = 57896044618658097711785492504343953926634992332820282019739415617116836954601\n",
    "# q = 57896044618658097711785492504343953926634992332820282019734762335316498173133\n",
    "n = p * q\n",
    "e = 65537\n",
    "\n",
    "print(\"ciphertext:\", c)\n",
    "\n",
    "# 1. Compute phi(n)\n",
    "phi_n = (p - 1) * (q - 1)\n",
    "\n",
    "# 2. Compute private key d\n",
    "d = mod_inverse(e, phi_n)\n",
    "\n",
    "# 3. Decrypt: m = c^d mod n\n",
    "m = pow(c, d, n)\n",
    "\n",
    "print(\"Decrypted numeric message:\", m)\n",
    "\n",
    "# 4. Convert numeric message into alphabetic string using mod 26\n",
    "m_str = str(m)\n",
    "if len(m_str) % 2 != 0:\n",
    "    m_str = '0' + m_str\n",
    "\n",
    "# Split into 2-digit chunks\n",
    "decoded_chars = []\n",
    "for i in range(0, len(m_str), 2):\n",
    "    val = int(m_str[i:i+2]) % 26\n",
    "    if val == 0:\n",
    "        val = 26\n",
    "    decoded_chars.append(chr(val + 64))  # 1->A, 2->B, ..., 26->Z\n",
    "\n",
    "decoded_message = ''.join(decoded_chars)\n",
    "print(\"Decrypted text message:\", decoded_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ffd96-2090-46c0-b8ed-88ab191d16dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
